# Vit-super-token-sampling
Vision Transformer enhanced with Super Token Sampling, RoPE, Conv-MLP, and full training pipeline with evaluation and visualizations.

# ğŸ”¬ Vision Transformer with Super Token Sampling (ViT-STS)

This repository contains a custom implementation of a **Vision Transformer** enhanced with:

- ğŸ§© Multi-scale patch embedding
- ğŸ“ Relative positional embeddings (RoPE)
- ğŸ§  Conv-based MLP layers
- ğŸŒŸ **Super Token Sampling (STS)** for token selection
- ğŸ“Š Extensive evaluation pipeline with classification reports, confusion matrices, and model statistics

## ğŸš€ Highlights

- âœ… Customizable ViT depth, head size, patch size
- ğŸ“¦ Full training, validation, and testing loop with checkpointing
- ğŸ§  Advanced metrics: Top-1, Top-3, Precision, Recall, F1, AUC
- ğŸ“‰ Visualizations: Loss curves, accuracy plots, confusion matrices
- ğŸ“Š Outputs:
- `trained_model.pth`
- `results.json`, `detailed_metrics.txt`, `classification_report.txt`
- `.png` plots for loss & confusion matrices

## ğŸ“ Project Structure
vit-super-token-sampling/
â”œâ”€â”€ Sts.py
â”œâ”€â”€ Images/ or SoyMCData/
â”œâ”€â”€ results/
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â””â”€â”€ .gitignore

## ğŸ› ï¸ Installation
pip install torch torchvision matplotlib seaborn scikit-learn tqdm ptflops

ğŸ“ˆ How to Run
Organize your dataset as:
SoyMCData/
â”œâ”€â”€ train/
â”œâ”€â”€ val/
â””â”€â”€ test/
Update save_dir in Sts.py.

Run training:
python Sts.py

ğŸ–¼ï¸ Example Outputs
loss_curves.png
top1_top3_comparison.png
test_confusion_matrix.png
model_statistics.json
trained_model.pth

